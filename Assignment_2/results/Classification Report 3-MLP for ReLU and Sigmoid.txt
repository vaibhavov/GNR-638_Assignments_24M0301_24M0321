Classification Report for ReLU and Sigmoid Activation with 256 Nodes:
              precision    recall  f1-score   support

           0       0.94      1.00      0.97        16
           1       0.83      0.71      0.77        21
           2       0.53      0.38      0.44        21
           3       0.72      0.86      0.78        21
           4       0.63      0.50      0.56        24
           5       0.94      1.00      0.97        16
           6       0.60      0.30      0.40        20
           7       0.82      0.88      0.85        16
           8       0.75      0.41      0.53        22
           9       0.70      0.70      0.70        20
          10       1.00      1.00      1.00        21
          11       0.56      0.68      0.61        22
          12       0.56      0.59      0.57        17
          13       0.59      0.93      0.72        14
          14       0.65      0.83      0.73        18
          15       0.96      0.96      0.96        26
          16       0.62      0.77      0.69        26
          17       0.84      0.80      0.82        20
          18       0.41      0.41      0.41        17
          19       0.50      0.44      0.47        25
          20       0.57      0.76      0.65        17

    accuracy                           0.70       420
   macro avg       0.70      0.71      0.70       420
weighted avg       0.70      0.70      0.69       420